{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-LPdeJnLbhP5"
   },
   "source": [
    "# Keras package in Python\n",
    "\n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
    "\n",
    "Use Keras if you need a deep learning library that:\n",
    "\n",
    "Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
    "Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
    "Runs seamlessly on CPU and GPU.\n",
    "Read the documentation at Keras.io.\n",
    "\n",
    "Keras is compatible with: Python 2.7-3.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j5qa9OtRbyqO"
   },
   "source": [
    "The core data structure of Keras is a **model**, a way to organize layers. The simplest type of model is the Sequential model, a linear stack of layers. For more complex architectures, you should use the Keras functional API, which allows to build arbitrary graphs of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F6zlphu1bZIC"
   },
   "outputs": [],
   "source": [
    "#Here is the Sequential model:\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PHysZQcdcAza"
   },
   "outputs": [],
   "source": [
    "#Stacking layers is as easy as .add():\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "model.add(Dense(units=1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fBcPLRoqd9iH"
   },
   "source": [
    "# Compilation\n",
    "\n",
    "Before training a model, you need to configure the learning process, which is done via the compile method. It receives three arguments:\n",
    "\n",
    "An optimizer. This could be the string identifier of an existing optimizer (such as rmsprop or adagrad), or an instance of the Optimizer class. \n",
    "\n",
    "\n",
    "A loss function. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as categorical_crossentropy or mse), or it can be an objective function. \n",
    "\n",
    "A list of metrics. For any classification problem you will want to set this to metrics=['accuracy']. A metric could be the string identifier of an existing metric or a custom metric function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XD6V0BrQcCNh"
   },
   "outputs": [],
   "source": [
    "#Once your model looks good, configure its learning process with .compile():\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XpokwhIudqU7"
   },
   "source": [
    "# Specifying the input shape\n",
    "\n",
    "The model needs to know what input shape it should expect. For this reason, the first layer in a Sequential model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape. There are several possible ways to do this:\n",
    "\n",
    "Pass an input_shape argument to the first layer. This is a shape tuple (a tuple of integers or None entries, where None indicates that any positive integer may be expected). In input_shape, the batch dimension is not included.\n",
    "\n",
    "Some 2D layers, such as Dense, support the specification of their input shape via the argument input_dim, and some 3D temporal layers support the arguments input_dim and input_length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0q7KHVXVd47_"
   },
   "outputs": [],
   "source": [
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(2, size=(1000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4T2tgwU0e-fX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of inputs\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b7wfi1JJfFtu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of labels\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yqYljCVFeLBj"
   },
   "source": [
    "# Training\n",
    "\n",
    "Keras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the fit function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZG96MZWdeP5F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 490us/step - loss: 7.8596 - acc: 0.5070\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 7.8596 - acc: 0.5070\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 7.8596 - acc: 0.5070\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 77us/step - loss: 7.8596 - acc: 0.5070\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 7.8596 - acc: 0.5070\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 96us/step - loss: 7.8596 - acc: 0.5070\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 80us/step - loss: 7.8596 - acc: 0.5070\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 7.8596 - acc: 0.5070\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 7.8596 - acc: 0.5070\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 7.8596 - acc: 0.5070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x291f48eecc8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUSaXF22foxp"
   },
   "source": [
    "# Exercise 1: Build a Sequential model with two layers. \n",
    "\n",
    "Layer 1 is dense with 32 nodes and 'relu' activation function. Input dimension is 100.\n",
    "\n",
    "Layer 2 is dense with 10 nodes and 'softmax' activation function.\n",
    "\n",
    "Compile the model with optimizer 'rmsprop',             loss='categorical_crossentropy' and metrics=['accuracy']\n",
    "\n",
    "\n",
    "Generate dummy data using np.random.random 1000, 100 and dummy labels of 10 categories with size 1000,1\n",
    "\n",
    "\n",
    "## Convert labels to categorical one-hot encoding using below statement\n",
    "from keras.utils import to_categorical\n",
    "one_hot_labels = to_categorical(labels, num_classes=10)\n",
    "\n",
    "Finally, Train the model, iterating on the data in batches of 32 samples\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PvbkQ7pKzO8Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 495us/step - loss: 2.3732 - acc: 0.1030\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 2.3168 - acc: 0.1080\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 2.3016 - acc: 0.1300\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 2.2903 - acc: 0.1220\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 2.2833 - acc: 0.1320\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 84us/step - loss: 2.2759 - acc: 0.1460\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 2.2699 - acc: 0.1580\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 2.2626 - acc: 0.1590\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 75us/step - loss: 2.2538 - acc: 0.1590\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 2.2450 - acc: 0.1770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x291f49fbd48>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(10, size=(1000, 1))\n",
    "\n",
    "one_hot_labels = to_categorical(labels, num_classes=10)\n",
    "\n",
    "model.fit(data, one_hot_labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sJrdnfAMfWbc"
   },
   "source": [
    "Double-click <b>here</b> for the solution.\n",
    "\n",
    "<!-- The answer is below:\n",
    "from keras.utils import to_categorical\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(10, size=(1000, 1))\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "one_hot_labels = to_categorical(labels, num_classes=10)\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, one_hot_labels, epochs=10, batch_size=32)\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kVpe2f24jTIW"
   },
   "source": [
    "# Exercise 2: Building a Basic Keras Neural Network Sequential Model to classify MNIST dataset\n",
    "\n",
    "First we import package and a set hyperparameter and identify dataset variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gl3kYpMSjqUO"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "\n",
    "NUM_ROWS = 28\n",
    "NUM_COLS = 28\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qr-2yrEUjt5Q"
   },
   "source": [
    "## Next is a function for outputting some simple (but useful) metadata of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "koiwyv_Jj0Yf"
   },
   "outputs": [],
   "source": [
    "def data_summary(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Summarize current state of dataset\"\"\"\n",
    "    print('Train images shape:', X_train.shape)\n",
    "    print('Train labels shape:', y_train.shape)\n",
    "    print('Test images shape:', X_test.shape)\n",
    "    print('Test labels shape:', y_test.shape)\n",
    "    print('Train labels:', y_train)\n",
    "    print('Test labels:', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DcHaDkegj-LU"
   },
   "source": [
    "## Next we load our dataset (MNIST, using Keras' dataset utilities), and then use the function above to get some dataset metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BeMLUCHHkBJy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (60000, 28, 28)\n",
      "Train labels shape: (60000,)\n",
      "Test images shape: (10000, 28, 28)\n",
      "Test labels shape: (10000,)\n",
      "Train labels: [5 0 4 ... 5 6 8]\n",
      "Test labels: [7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Check state of dataset\n",
    "data_summary(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CQBMRuA5kKTQ"
   },
   "source": [
    "To feed MNIST instances into a neural network, they need to be reshaped, from a 2 dimensional image representation to a single dimension sequence. We also convert our class vector to a binary matrix (using to_categorical). This is accomplished below, after which the same function defined above is called again in order to show the effects of our data reshaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fRwDGnB8kLpm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (60000, 784)\n",
      "Train labels shape: (60000, 10)\n",
      "Test images shape: (10000, 784)\n",
      "Test labels shape: (10000, 10)\n",
      "Train labels: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "Test labels: [[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Reshape data\n",
    "X_train = X_train.reshape((X_train.shape[0], NUM_ROWS * NUM_COLS))\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.reshape((X_test.shape[0], NUM_ROWS * NUM_COLS))\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "# Categorically encode labels\n",
    "y_train = to_categorical(y_train, NUM_CLASSES)\n",
    "y_test = to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "# Check state of dataset\n",
    "data_summary(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ho-Gqe4elgoK"
   },
   "source": [
    "#Build a Sequential model of Neural Network with three dense layers.\n",
    "\n",
    "\n",
    "\n",
    "1.   Layer 1, 2 and 3 have 512, 256 and 10 nodes respectively. Activation function of first two layers is relu and final layer is softmax.\n",
    "2.   Compile model with optimizer='rmsprop',               loss='categorical_crossentropy' and metrics=['accuracy']\n",
    "3. Train the model with two additional parameters verbose=1 and validation_data=(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4Q8WK4BB_r9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.2283 - acc: 0.9289 - val_loss: 0.1171 - val_acc: 0.9634\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.0847 - acc: 0.9738 - val_loss: 0.0872 - val_acc: 0.9737\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.0551 - acc: 0.9830 - val_loss: 0.0839 - val_acc: 0.9759\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.0397 - acc: 0.9876 - val_loss: 0.0867 - val_acc: 0.9758\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 0.0302 - acc: 0.9906 - val_loss: 0.0869 - val_acc: 0.9789\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.0235 - acc: 0.9927 - val_loss: 0.0778 - val_acc: 0.9813\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.0188 - acc: 0.9942 - val_loss: 0.1020 - val_acc: 0.9776\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0143 - acc: 0.9955 - val_loss: 0.1020 - val_acc: 0.9778\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.0124 - acc: 0.9960 - val_loss: 0.0947 - val_acc: 0.9810\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.0104 - acc: 0.9969 - val_loss: 0.0926 - val_acc: 0.9824\n",
      "Test loss: 0.09262872967608542\n",
      "Test accuracy: 0.9824\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F2ajV6BzmA8E"
   },
   "source": [
    "Double-click <b>here</b> for the solution.\n",
    "\n",
    "<!-- The answer is below:\n",
    "# Build neural network\n",
    "model = models.Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_R1P-fE9mlkV"
   },
   "source": [
    "#Output a summary of the neural network we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XF62Moggmnye"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 535,818\n",
      "Trainable params: 535,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary of neural network\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B3r9ODkFmqwX"
   },
   "source": [
    "# Visualize the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pF5PEZWPmtvf"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 119.00 264.00\" width=\"119pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-260 115,-260 115,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 2001518395208 -->\n",
       "<g class=\"node\" id=\"node1\"><title>2001518395208</title>\n",
       "<polygon fill=\"none\" points=\"3.5,-146.5 3.5,-182.5 107.5,-182.5 107.5,-146.5 3.5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"55.5\" y=\"-160.8\">dense_9: Dense</text>\n",
       "</g>\n",
       "<!-- 2001519384968 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2001519384968</title>\n",
       "<polygon fill=\"none\" points=\"0,-73.5 0,-109.5 111,-109.5 111,-73.5 0,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"55.5\" y=\"-87.8\">dense_10: Dense</text>\n",
       "</g>\n",
       "<!-- 2001518395208&#45;&gt;2001519384968 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>2001518395208-&gt;2001519384968</title>\n",
       "<path d=\"M55.5,-146.313C55.5,-138.289 55.5,-128.547 55.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"59.0001,-119.529 55.5,-109.529 52.0001,-119.529 59.0001,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2001519678792 -->\n",
       "<g class=\"node\" id=\"node3\"><title>2001519678792</title>\n",
       "<polygon fill=\"none\" points=\"0,-0.5 0,-36.5 111,-36.5 111,-0.5 0,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"55.5\" y=\"-14.8\">dense_11: Dense</text>\n",
       "</g>\n",
       "<!-- 2001519384968&#45;&gt;2001519678792 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>2001519384968-&gt;2001519678792</title>\n",
       "<path d=\"M55.5,-73.3129C55.5,-65.2895 55.5,-55.5475 55.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"59.0001,-46.5288 55.5,-36.5288 52.0001,-46.5289 59.0001,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2001519499080 -->\n",
       "<g class=\"node\" id=\"node4\"><title>2001519499080</title>\n",
       "<polygon fill=\"none\" points=\"3.5,-219.5 3.5,-255.5 107.5,-255.5 107.5,-219.5 3.5,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"55.5\" y=\"-233.8\">2001519499080</text>\n",
       "</g>\n",
       "<!-- 2001519499080&#45;&gt;2001518395208 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2001519499080-&gt;2001518395208</title>\n",
       "<path d=\"M55.5,-219.313C55.5,-211.289 55.5,-201.547 55.5,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"59.0001,-192.529 55.5,-182.529 52.0001,-192.529 59.0001,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydot as pyd\n",
    "\n",
    "keras.utils.vis_utils.pydot = pyd\n",
    "# Output network visualization\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-XeWXc0ft6fO"
   },
   "source": [
    "#Exercise: Train a Sequential Neural network on CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tPWjNVe9uCnw"
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pjXMCDsxuHAc"
   },
   "outputs": [],
   "source": [
    "#import dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "#change shape from image to vector\n",
    "X_train = X_train.reshape(50000, 32 * 32 * 3)\n",
    "X_test = X_test.reshape(10000, 32 * 32 * 3)\n",
    "\n",
    "#preprocess\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "\n",
    "#change labels from numeric to one hot encoded\n",
    "Y_train = to_categorical(y_train, 10)\n",
    "Y_test =  to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0SV6PPCHu6iy"
   },
   "source": [
    "# Build sequential model \n",
    "\n",
    "\n",
    "\n",
    "1.   Sequential model with four layers having 1024,512,512,10 nodes respectively\n",
    "2.   Input shape = 3072\n",
    "3.   Activation of first three layers is relu and final is softmax\n",
    "4.   Compile model using loss='categorical_crossentropy', optimizer='adam' and metrics=['accuracy']\n",
    "5. Train model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bo-lCLhHB4YL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "50000/50000 [==============================] - 19s 372us/step - loss: 1.8918 - acc: 0.3200 - val_loss: 1.7532 - val_acc: 0.3550\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 18s 356us/step - loss: 1.6678 - acc: 0.4006 - val_loss: 1.6566 - val_acc: 0.3978\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 17s 345us/step - loss: 1.5792 - acc: 0.4338 - val_loss: 1.5570 - val_acc: 0.4481\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 17s 344us/step - loss: 1.5237 - acc: 0.4529 - val_loss: 1.4959 - val_acc: 0.4642\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 17s 343us/step - loss: 1.4661 - acc: 0.4752 - val_loss: 1.5035 - val_acc: 0.4612\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 17s 350us/step - loss: 1.4194 - acc: 0.4949 - val_loss: 1.4639 - val_acc: 0.4780\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 17s 338us/step - loss: 1.3936 - acc: 0.5030 - val_loss: 1.4462 - val_acc: 0.4830\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 18s 360us/step - loss: 1.3601 - acc: 0.5133 - val_loss: 1.4302 - val_acc: 0.4881\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 17s 344us/step - loss: 1.3200 - acc: 0.5296 - val_loss: 1.4065 - val_acc: 0.5021\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 17s 347us/step - loss: 1.2924 - acc: 0.5378 - val_loss: 1.4030 - val_acc: 0.5121\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, input_shape=(3072, )))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                        batch_size=128,\n",
    "                        nb_epoch=10,\n",
    "                        verbose=1,\n",
    "                        validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t51-PrDOvK0R"
   },
   "source": [
    "Double-click <b>here</b> for the solution.\n",
    "\n",
    "<!--\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_shape=(3072, )))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    " \n",
    "\n",
    "# training\n",
    "history = model.fit(X_train, Y_train,\n",
    "                        batch_size=128,\n",
    "                        nb_epoch=10,\n",
    "                        verbose=1,\n",
    "                        validation_data=(X_test, Y_test))\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c81PL1bVxZRZ"
   },
   "source": [
    "# Plot accuracy and loss after model is trained. \n",
    "\n",
    "(Caution: Will not work unless previous step is performed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ptYUQKCWwTUf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U1JOX52A01Dc"
   },
   "source": [
    "# Improving accuracy on Iris dataset\n",
    "\n",
    "\n",
    "\n",
    "1.   Find the accuracy of the below model\n",
    "2.   Improve accuracy by increasing layers, epochs, optimizer or loss metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PN6DlpbYytgU",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 1s - loss: 1.2707 - acc: 0.3378\n",
      "Epoch 2/200\n",
      " - 0s - loss: 1.0706 - acc: 0.3446\n",
      "Epoch 3/200\n",
      " - 0s - loss: 1.0545 - acc: 0.3514\n",
      "Epoch 4/200\n",
      " - 0s - loss: 1.0488 - acc: 0.3446\n",
      "Epoch 5/200\n",
      " - 0s - loss: 1.0335 - acc: 0.4324\n",
      "Epoch 6/200\n",
      " - 0s - loss: 1.0175 - acc: 0.3446\n",
      "Epoch 7/200\n",
      " - 0s - loss: 1.0007 - acc: 0.3446\n",
      "Epoch 8/200\n",
      " - 0s - loss: 0.9893 - acc: 0.3716\n",
      "Epoch 9/200\n",
      " - 0s - loss: 0.9667 - acc: 0.3514\n",
      "Epoch 10/200\n",
      " - 0s - loss: 0.9607 - acc: 0.4122\n",
      "Epoch 11/200\n",
      " - 0s - loss: 0.9338 - acc: 0.3446\n",
      "Epoch 12/200\n",
      " - 0s - loss: 0.9192 - acc: 0.3784\n",
      "Epoch 13/200\n",
      " - 0s - loss: 0.8934 - acc: 0.3581\n",
      "Epoch 14/200\n",
      " - 0s - loss: 0.8768 - acc: 0.3514\n",
      "Epoch 15/200\n",
      " - 0s - loss: 0.8526 - acc: 0.3514\n",
      "Epoch 16/200\n",
      " - 0s - loss: 0.8262 - acc: 0.3649\n",
      "Epoch 17/200\n",
      " - 0s - loss: 0.8043 - acc: 0.5068\n",
      "Epoch 18/200\n",
      " - 0s - loss: 0.7799 - acc: 0.6554\n",
      "Epoch 19/200\n",
      " - 0s - loss: 0.7577 - acc: 0.8041\n",
      "Epoch 20/200\n",
      " - 0s - loss: 0.7285 - acc: 0.6959\n",
      "Epoch 21/200\n",
      " - 0s - loss: 0.7144 - acc: 0.8716\n",
      "Epoch 22/200\n",
      " - 0s - loss: 0.6842 - acc: 0.6959\n",
      "Epoch 23/200\n",
      " - 0s - loss: 0.6475 - acc: 0.7635\n",
      "Epoch 24/200\n",
      " - 0s - loss: 0.6247 - acc: 0.7635\n",
      "Epoch 25/200\n",
      " - 0s - loss: 0.5973 - acc: 0.7297\n",
      "Epoch 26/200\n",
      " - 0s - loss: 0.5827 - acc: 0.8716\n",
      "Epoch 27/200\n",
      " - 0s - loss: 0.5536 - acc: 0.7432\n",
      "Epoch 28/200\n",
      " - 0s - loss: 0.5306 - acc: 0.8851\n",
      "Epoch 29/200\n",
      " - 0s - loss: 0.5134 - acc: 0.7703\n",
      "Epoch 30/200\n",
      " - 0s - loss: 0.4978 - acc: 0.9189\n",
      "Epoch 31/200\n",
      " - 0s - loss: 0.4798 - acc: 0.8108\n",
      "Epoch 32/200\n",
      " - 0s - loss: 0.4644 - acc: 0.8514\n",
      "Epoch 33/200\n",
      " - 0s - loss: 0.4503 - acc: 0.8581\n",
      "Epoch 34/200\n",
      " - 0s - loss: 0.4350 - acc: 0.8919\n",
      "Epoch 35/200\n",
      " - 0s - loss: 0.4248 - acc: 0.9189\n",
      "Epoch 36/200\n",
      " - 0s - loss: 0.4181 - acc: 0.8378\n",
      "Epoch 37/200\n",
      " - 0s - loss: 0.4032 - acc: 0.9459\n",
      "Epoch 38/200\n",
      " - 0s - loss: 0.3939 - acc: 0.8649\n",
      "Epoch 39/200\n",
      " - 0s - loss: 0.3892 - acc: 0.9797\n",
      "Epoch 40/200\n",
      " - 0s - loss: 0.3768 - acc: 0.8514\n",
      "Epoch 41/200\n",
      " - 0s - loss: 0.3730 - acc: 0.9595\n",
      "Epoch 42/200\n",
      " - 0s - loss: 0.3591 - acc: 0.9392\n",
      "Epoch 43/200\n",
      " - 0s - loss: 0.3485 - acc: 0.9392\n",
      "Epoch 44/200\n",
      " - 0s - loss: 0.3445 - acc: 0.9730\n",
      "Epoch 45/200\n",
      " - 0s - loss: 0.3391 - acc: 0.9392\n",
      "Epoch 46/200\n",
      " - 0s - loss: 0.3250 - acc: 0.9392\n",
      "Epoch 47/200\n",
      " - 0s - loss: 0.3197 - acc: 0.9662\n",
      "Epoch 48/200\n",
      " - 0s - loss: 0.3181 - acc: 0.9392\n",
      "Epoch 49/200\n",
      " - 0s - loss: 0.3020 - acc: 0.9662\n",
      "Epoch 50/200\n",
      " - 0s - loss: 0.2949 - acc: 0.9730\n",
      "Epoch 51/200\n",
      " - 0s - loss: 0.2947 - acc: 0.9595\n",
      "Epoch 52/200\n",
      " - 0s - loss: 0.2820 - acc: 0.9595\n",
      "Epoch 53/200\n",
      " - 0s - loss: 0.2757 - acc: 0.9662\n",
      "Epoch 54/200\n",
      " - 0s - loss: 0.2698 - acc: 0.9595\n",
      "Epoch 55/200\n",
      " - 0s - loss: 0.2753 - acc: 0.9324\n",
      "Epoch 56/200\n",
      " - 0s - loss: 0.2586 - acc: 0.9392\n",
      "Epoch 57/200\n",
      " - 0s - loss: 0.2502 - acc: 0.9662\n",
      "Epoch 58/200\n",
      " - 0s - loss: 0.2452 - acc: 0.9662\n",
      "Epoch 59/200\n",
      " - 0s - loss: 0.2373 - acc: 0.9797\n",
      "Epoch 60/200\n",
      " - 0s - loss: 0.2337 - acc: 0.9730\n",
      "Epoch 61/200\n",
      " - 0s - loss: 0.2281 - acc: 0.9797\n",
      "Epoch 62/200\n",
      " - 0s - loss: 0.2245 - acc: 0.9595\n",
      "Epoch 63/200\n",
      " - 0s - loss: 0.2209 - acc: 0.9527\n",
      "Epoch 64/200\n",
      " - 0s - loss: 0.2087 - acc: 0.9797\n",
      "Epoch 65/200\n",
      " - 0s - loss: 0.2084 - acc: 0.9730\n",
      "Epoch 66/200\n",
      " - 0s - loss: 0.2044 - acc: 0.9662\n",
      "Epoch 67/200\n",
      " - 0s - loss: 0.2051 - acc: 0.9662\n",
      "Epoch 68/200\n",
      " - 0s - loss: 0.1957 - acc: 0.9730\n",
      "Epoch 69/200\n",
      " - 0s - loss: 0.1940 - acc: 0.9730\n",
      "Epoch 70/200\n",
      " - 0s - loss: 0.1839 - acc: 0.9797\n",
      "Epoch 71/200\n",
      " - 0s - loss: 0.1824 - acc: 0.9730\n",
      "Epoch 72/200\n",
      " - 0s - loss: 0.1769 - acc: 0.9797\n",
      "Epoch 73/200\n",
      " - 0s - loss: 0.1737 - acc: 0.9797\n",
      "Epoch 74/200\n",
      " - 0s - loss: 0.1725 - acc: 0.9730\n",
      "Epoch 75/200\n",
      " - 0s - loss: 0.1737 - acc: 0.9595\n",
      "Epoch 76/200\n",
      " - 0s - loss: 0.1625 - acc: 0.9527\n",
      "Epoch 77/200\n",
      " - 0s - loss: 0.1626 - acc: 0.9730\n",
      "Epoch 78/200\n",
      " - 0s - loss: 0.1603 - acc: 0.9662\n",
      "Epoch 79/200\n",
      " - 0s - loss: 0.1526 - acc: 0.9797\n",
      "Epoch 80/200\n",
      " - 0s - loss: 0.1497 - acc: 0.9797\n",
      "Epoch 81/200\n",
      " - 0s - loss: 0.1480 - acc: 0.9797\n",
      "Epoch 82/200\n",
      " - 0s - loss: 0.1473 - acc: 0.9730\n",
      "Epoch 83/200\n",
      " - 0s - loss: 0.1456 - acc: 0.9730\n",
      "Epoch 84/200\n",
      " - 0s - loss: 0.1415 - acc: 0.9662\n",
      "Epoch 85/200\n",
      " - 0s - loss: 0.1366 - acc: 0.9730\n",
      "Epoch 86/200\n",
      " - 0s - loss: 0.1372 - acc: 0.9797\n",
      "Epoch 87/200\n",
      " - 0s - loss: 0.1346 - acc: 0.9797\n",
      "Epoch 88/200\n",
      " - 0s - loss: 0.1359 - acc: 0.9730\n",
      "Epoch 89/200\n",
      " - 0s - loss: 0.1299 - acc: 0.9797\n",
      "Epoch 90/200\n",
      " - 0s - loss: 0.1262 - acc: 0.9797\n",
      "Epoch 91/200\n",
      " - 0s - loss: 0.1283 - acc: 0.9730\n",
      "Epoch 92/200\n",
      " - 0s - loss: 0.1245 - acc: 0.9730\n",
      "Epoch 93/200\n",
      " - 0s - loss: 0.1232 - acc: 0.9730\n",
      "Epoch 94/200\n",
      " - 0s - loss: 0.1258 - acc: 0.9595\n",
      "Epoch 95/200\n",
      " - 0s - loss: 0.1198 - acc: 0.9662\n",
      "Epoch 96/200\n",
      " - 0s - loss: 0.1188 - acc: 0.9797\n",
      "Epoch 97/200\n",
      " - 0s - loss: 0.1194 - acc: 0.9730\n",
      "Epoch 98/200\n",
      " - 0s - loss: 0.1179 - acc: 0.9730\n",
      "Epoch 99/200\n",
      " - 0s - loss: 0.1125 - acc: 0.9730\n",
      "Epoch 100/200\n",
      " - 0s - loss: 0.1154 - acc: 0.9730\n",
      "Epoch 101/200\n",
      " - 0s - loss: 0.1102 - acc: 0.9797\n",
      "Epoch 102/200\n",
      " - 0s - loss: 0.1092 - acc: 0.9797\n",
      "Epoch 103/200\n",
      " - 0s - loss: 0.1072 - acc: 0.9730\n",
      "Epoch 104/200\n",
      " - 0s - loss: 0.1134 - acc: 0.9595\n",
      "Epoch 105/200\n",
      " - 0s - loss: 0.1071 - acc: 0.9730\n",
      "Epoch 106/200\n",
      " - 0s - loss: 0.1053 - acc: 0.9730\n",
      "Epoch 107/200\n",
      " - 0s - loss: 0.1026 - acc: 0.9797\n",
      "Epoch 108/200\n",
      " - 0s - loss: 0.1075 - acc: 0.9662\n",
      "Epoch 109/200\n",
      " - 0s - loss: 0.1020 - acc: 0.9730\n",
      "Epoch 110/200\n",
      " - 0s - loss: 0.1009 - acc: 0.9797\n",
      "Epoch 111/200\n",
      " - 0s - loss: 0.0995 - acc: 0.9797\n",
      "Epoch 112/200\n",
      " - 0s - loss: 0.0990 - acc: 0.9730\n",
      "Epoch 113/200\n",
      " - 0s - loss: 0.1007 - acc: 0.9797\n",
      "Epoch 114/200\n",
      " - 0s - loss: 0.0976 - acc: 0.9730\n",
      "Epoch 115/200\n",
      " - 0s - loss: 0.0948 - acc: 0.9797\n",
      "Epoch 116/200\n",
      " - 0s - loss: 0.0974 - acc: 0.9797\n",
      "Epoch 117/200\n",
      " - 0s - loss: 0.1058 - acc: 0.9730\n",
      "Epoch 118/200\n",
      " - 0s - loss: 0.0922 - acc: 0.9662\n",
      "Epoch 119/200\n",
      " - 0s - loss: 0.0917 - acc: 0.9662\n",
      "Epoch 120/200\n",
      " - 0s - loss: 0.0936 - acc: 0.9797\n",
      "Epoch 121/200\n",
      " - 0s - loss: 0.0937 - acc: 0.9730\n",
      "Epoch 122/200\n",
      " - 0s - loss: 0.0994 - acc: 0.9730\n",
      "Epoch 123/200\n",
      " - 0s - loss: 0.0893 - acc: 0.9797\n",
      "Epoch 124/200\n",
      " - 0s - loss: 0.0891 - acc: 0.9797\n",
      "Epoch 125/200\n",
      " - 0s - loss: 0.0950 - acc: 0.9730\n",
      "Epoch 126/200\n",
      " - 0s - loss: 0.1049 - acc: 0.9595\n",
      "Epoch 127/200\n",
      " - 0s - loss: 0.0885 - acc: 0.9730\n",
      "Epoch 128/200\n",
      " - 0s - loss: 0.0921 - acc: 0.9595\n",
      "Epoch 129/200\n",
      " - 0s - loss: 0.0936 - acc: 0.9797\n",
      "Epoch 130/200\n",
      " - 0s - loss: 0.0880 - acc: 0.9662\n",
      "Epoch 131/200\n",
      " - 0s - loss: 0.0848 - acc: 0.9730\n",
      "Epoch 132/200\n",
      " - 0s - loss: 0.0884 - acc: 0.9730\n",
      "Epoch 133/200\n",
      " - 0s - loss: 0.0863 - acc: 0.9730\n",
      "Epoch 134/200\n",
      " - 0s - loss: 0.0835 - acc: 0.9797\n",
      "Epoch 135/200\n",
      " - 0s - loss: 0.0848 - acc: 0.9797\n",
      "Epoch 136/200\n",
      " - 0s - loss: 0.0836 - acc: 0.9797\n",
      "Epoch 137/200\n",
      " - 0s - loss: 0.0906 - acc: 0.9595\n",
      "Epoch 138/200\n",
      " - 0s - loss: 0.0876 - acc: 0.9730\n",
      "Epoch 139/200\n",
      " - 0s - loss: 0.0967 - acc: 0.9662\n",
      "Epoch 140/200\n",
      " - 0s - loss: 0.0841 - acc: 0.9730\n",
      "Epoch 141/200\n",
      " - 0s - loss: 0.0840 - acc: 0.9797\n",
      "Epoch 142/200\n",
      " - 0s - loss: 0.0813 - acc: 0.9662\n",
      "Epoch 143/200\n",
      " - 0s - loss: 0.0874 - acc: 0.9662\n",
      "Epoch 144/200\n",
      " - 0s - loss: 0.0853 - acc: 0.9662\n",
      "Epoch 145/200\n",
      " - 0s - loss: 0.0861 - acc: 0.9730\n",
      "Epoch 146/200\n",
      " - 0s - loss: 0.0827 - acc: 0.9730\n",
      "Epoch 147/200\n",
      " - 0s - loss: 0.0810 - acc: 0.9797\n",
      "Epoch 148/200\n",
      " - 0s - loss: 0.0816 - acc: 0.9662\n",
      "Epoch 149/200\n",
      " - 0s - loss: 0.0815 - acc: 0.9730\n",
      "Epoch 150/200\n",
      " - 0s - loss: 0.0815 - acc: 0.9730\n",
      "Epoch 151/200\n",
      " - 0s - loss: 0.0778 - acc: 0.9797\n",
      "Epoch 152/200\n",
      " - 0s - loss: 0.0776 - acc: 0.9797\n",
      "Epoch 153/200\n",
      " - 0s - loss: 0.0776 - acc: 0.9797\n",
      "Epoch 154/200\n",
      " - 0s - loss: 0.0804 - acc: 0.9730\n",
      "Epoch 155/200\n",
      " - 0s - loss: 0.0812 - acc: 0.9730\n",
      "Epoch 156/200\n",
      " - 0s - loss: 0.0746 - acc: 0.9730\n",
      "Epoch 157/200\n",
      " - 0s - loss: 0.0764 - acc: 0.9797\n",
      "Epoch 158/200\n",
      " - 0s - loss: 0.0757 - acc: 0.9730\n",
      "Epoch 159/200\n",
      " - 0s - loss: 0.0811 - acc: 0.9662\n",
      "Epoch 160/200\n",
      " - 0s - loss: 0.0753 - acc: 0.9662\n",
      "Epoch 161/200\n",
      " - 0s - loss: 0.0752 - acc: 0.9730\n",
      "Epoch 162/200\n",
      " - 0s - loss: 0.0749 - acc: 0.9797\n",
      "Epoch 163/200\n",
      " - 0s - loss: 0.0767 - acc: 0.9730\n",
      "Epoch 164/200\n",
      " - 0s - loss: 0.0754 - acc: 0.9730\n",
      "Epoch 165/200\n",
      " - 0s - loss: 0.0765 - acc: 0.9662\n",
      "Epoch 166/200\n",
      " - 0s - loss: 0.0730 - acc: 0.9797\n",
      "Epoch 167/200\n",
      " - 0s - loss: 0.0712 - acc: 0.9797\n",
      "Epoch 168/200\n",
      " - 0s - loss: 0.0760 - acc: 0.9730\n",
      "Epoch 169/200\n",
      " - 0s - loss: 0.0766 - acc: 0.9730\n",
      "Epoch 170/200\n",
      " - 0s - loss: 0.0754 - acc: 0.9730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/200\n",
      " - 0s - loss: 0.0757 - acc: 0.9730\n",
      "Epoch 172/200\n",
      " - 0s - loss: 0.0771 - acc: 0.9797\n",
      "Epoch 173/200\n",
      " - 0s - loss: 0.0789 - acc: 0.9730\n",
      "Epoch 174/200\n",
      " - 0s - loss: 0.0763 - acc: 0.9797\n",
      "Epoch 175/200\n",
      " - 0s - loss: 0.0800 - acc: 0.9797\n",
      "Epoch 176/200\n",
      " - 0s - loss: 0.0739 - acc: 0.9730\n",
      "Epoch 177/200\n",
      " - 0s - loss: 0.0730 - acc: 0.9797\n",
      "Epoch 178/200\n",
      " - 0s - loss: 0.0708 - acc: 0.9730\n",
      "Epoch 179/200\n",
      " - 0s - loss: 0.0718 - acc: 0.9730\n",
      "Epoch 180/200\n",
      " - 0s - loss: 0.0816 - acc: 0.9797\n",
      "Epoch 181/200\n",
      " - 0s - loss: 0.0716 - acc: 0.9730\n",
      "Epoch 182/200\n",
      " - 0s - loss: 0.0734 - acc: 0.9662\n",
      "Epoch 183/200\n",
      " - 0s - loss: 0.0709 - acc: 0.9797\n",
      "Epoch 184/200\n",
      " - 0s - loss: 0.0767 - acc: 0.9797\n",
      "Epoch 185/200\n",
      " - 0s - loss: 0.0680 - acc: 0.9797\n",
      "Epoch 186/200\n",
      " - 0s - loss: 0.0749 - acc: 0.9595\n",
      "Epoch 187/200\n",
      " - 0s - loss: 0.0749 - acc: 0.9595\n",
      "Epoch 188/200\n",
      " - 0s - loss: 0.0752 - acc: 0.9662\n",
      "Epoch 189/200\n",
      " - 0s - loss: 0.0682 - acc: 0.9797\n",
      "Epoch 190/200\n",
      " - 0s - loss: 0.0709 - acc: 0.9797\n",
      "Epoch 191/200\n",
      " - 0s - loss: 0.0692 - acc: 0.9730\n",
      "Epoch 192/200\n",
      " - 0s - loss: 0.0716 - acc: 0.9730\n",
      "Epoch 193/200\n",
      " - 0s - loss: 0.0839 - acc: 0.9730\n",
      "Epoch 194/200\n",
      " - 0s - loss: 0.0796 - acc: 0.9595\n",
      "Epoch 195/200\n",
      " - 0s - loss: 0.0656 - acc: 0.9730\n",
      "Epoch 196/200\n",
      " - 0s - loss: 0.0722 - acc: 0.9730\n",
      "Epoch 197/200\n",
      " - 0s - loss: 0.0686 - acc: 0.9730\n",
      "Epoch 198/200\n",
      " - 0s - loss: 0.0677 - acc: 0.9797\n",
      "Epoch 199/200\n",
      " - 0s - loss: 0.0658 - acc: 0.9730\n",
      "Epoch 200/200\n",
      " - 0s - loss: 0.0680 - acc: 0.9730\n",
      "2/2 [==============================] - 0s 50ms/step\n",
      "Final test set loss: 0.002480\n",
      "Final test set accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "iris_data = load_iris()\n",
    "\t\n",
    "x = iris_data.data\n",
    "y_ = iris_data.target.reshape(-1, 1) # Convert data to a single column\n",
    "\n",
    "# One Hot encode the class labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y_)\n",
    "\n",
    "\n",
    "# Split the data for training and testing\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.01)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(10, input_shape=(4,), activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_x, train_y, verbose=2, batch_size=5, epochs=200)\n",
    "\n",
    "# Test on unseen data\n",
    "\n",
    "results = model.evaluate(test_x, test_y)\n",
    "\n",
    "print('Final test set loss: {:4f}'.format(results[0]))\n",
    "print('Final test set accuracy: {:4f}'.format(results[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-rkZZKV4yV9U"
   },
   "source": [
    "#Thank you for completing this notebook.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Simple-Keras.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
